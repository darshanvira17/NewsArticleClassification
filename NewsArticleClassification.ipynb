{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP41680 - Text Scraping & Classification Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Name:DARSHAN HARESH VIRA</b>\n",
    "\n",
    "<b>Student ID: 17204812</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment is about scraping the articles and categories and article classification from:\n",
    "http://mlg.ucd.ie/modules/COMP41680/archive/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b><u>Goal: Collect a labelled news corpus.<b></u></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I divided the PART 1, into subtasks.\n",
    "<p><u>Task 1:</u> Using beautiful soup I scraped the first page and collected all the links to articles arranged by the site month-wise.( January, February, .... )</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month-jan-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-jan-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-feb-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-mar-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-apr-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-may-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-jun-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-jul-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-aug-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-sep-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-oct-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-nov-2017.html\n",
      "http://mlg.ucd.ie/modules/COMP41680/archive/month-dec-2017.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "page = requests.get(\"http://mlg.ucd.ie/modules/COMP41680/archive/index.html\") #accessing the page to be scraped. \n",
    "soup = BeautifulSoup(page.content, 'html.parser') #parsing the html source cide\n",
    "#print(soup.prettify()) #dispalying html in a structured format\n",
    "a=soup.find_all('a')#this returns a list of all links(a tags)\n",
    "month_link=[]\n",
    "all_cat=[]\n",
    "print(a[0]['href']) #this shows that the link extension is scraped not the entire link\n",
    "for i in a:\n",
    "    URL = 'http://mlg.ucd.ie/modules/COMP41680/archive/' #main URL\n",
    "    link = i['href']\n",
    "    if(len(link)>2): #as it even has last two <a> tags without any href attributes which are of no use anyway.\n",
    "        URL+=link #constructs the entire link for each month\n",
    "        month_link.append(URL) #adds each link to a list\n",
    "for i in month_link:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><u>Task 2:</u> Moving to the second page, which contains a table with two columns, one is category of the article, and other one is the link to article. There are about 110-125 articles on each of the month page. So in this task I scarped the category of the article and link to article main body. From the link to the article, I moved to the next page and extracted all the content of each article and saved it in a text file. I used the article id present in each URL to name the article txt file of each article. I stored the article title and its category in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_id=[] #list of article ids\n",
    "categories=[] #stores the categories for all the\n",
    "title=[] #stores title of all articles\n",
    "for j in month_link: \n",
    "    links2=[] #stores the links of page 2\n",
    "    page2 = requests.get(j) #getting the page for each month link\n",
    "    soup2 = BeautifulSoup(page2.content,'html.parser') #parsing its HTML code\n",
    "    all_tds=soup2.find_all('td') #page is in table structure, so accessing td tag\n",
    "    all_tds=all_tds[2:] #discarding the column titles\n",
    "    for i in range(0,len(all_tds),2): #first column is Category so every EVEN element of all_tds list is Category \n",
    "        #for each td, we need to find the category text\n",
    "        var=all_tds[i].get_text().strip() # strip is used as N/A values had some spaces trailing and leading as well.\n",
    "        if var!='N/A':\n",
    "            categories.append(var) #so for each page, Categories list will have all the categories.\n",
    "    for i in range(1,len(all_tds),2): #Second column has link to artciles,so it is every ODD element of all_tds list \n",
    "        URL2 = 'http://mlg.ucd.ie/modules/COMP41680/archive/'\n",
    "        a=all_tds[i].find('a') #link to articles\n",
    "        if(a!=None): #some articles are removed\n",
    "            article_id.append(a['href'][8:16]) #out of href 8:16 corresponds to, 'jan-0418'(used as article id)\n",
    "            URL2+=a['href'] #constructing url\n",
    "            links2.append(URL2)\n",
    "    for l in links2:\n",
    "        page3 = requests.get(l) #getting each article\n",
    "        soup3 = BeautifulSoup(page3.content,'html.parser')\n",
    "        title.append(soup3.find('h2').get_text())#getting the artcile titles\n",
    "        content=soup3.find_all(lambda p: p.name=='p' and not p.attrs)#as some articles have <p> tags for other \n",
    "        #links or comments which are not part of the article.\n",
    "        final_content=\"\"\n",
    "        count=0\n",
    "        for i in content:\n",
    "            final_content+=i.get_text() #getting each text content and adding it to a string \n",
    "            file = open(str(l[52:60])+\".txt\",'w',encoding=\"utf-8\") #creating a file name with article ids eg.'jan-0418.txt'\n",
    "            file.write(final_content) #writing body of each article to a txt file\n",
    "            file.close()\n",
    "final_categories=pd.DataFrame()\n",
    "final_categories['article_id']=article_id\n",
    "final_categories['category']=categories\n",
    "final_categories['title']=title\n",
    "final_categories.to_csv(\"Category.csv\") #saving the categories, articles id and titles to excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b><u>Goal: Pre-process the data and build two classifiers and compare the results.<b></u></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the collection and storage of the article body and category, we now move forward to part 2 i.e. classifying each article belonging to Technology, Sports, Business, etc. \n",
    "The following task is divided into subtasks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From  the  files  created  in  Part  1, we now load the data back as raw documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1408 raw text documents\n",
      "The sporting industry has come a long way since the ‘60s. It has carved out for itself a niche with its roots so deep that I cannot fathom the sports industry showing any sign of decline any time soon - or later.The reason can be found in this seemingly subtle difference - other industries have customers; the sporting industry has fans. Vivek Ranadivé, leader of the ownership group of the NBA’s Sacramento Kings, explained it beautifully, “Fans will paint their face purple, fans will evangelize. ... Every other CEO in every business is dying to be in our position — they’re dying to have fans.“While fan passion alone could almost certainly keep the industry going, leagues and sporting franchises have decided not to rest on their laurels. The last few years have seen the steady introduction of technology into the world of sports - amplifying fans’ appreciation of games, enhancing athletes’ public profiles and informing their training methods, even influencing how contests are waged.Also, digital technology in particular has helped to create an alternative source of revenue, besides the games themselves - corporate sponsorship. They achieved this by capitalizing on the ardor of their customer base - sorry, fan base.\n"
     ]
    }
   ],
   "source": [
    "raw_documents=[] \n",
    "txtname=[] #to store the .txt name of the files\n",
    "for i in article_id:\n",
    "    txtname.append(i+\".txt\") #article_id+.txt makes up the file name\n",
    "for i in txtname:\n",
    "    fin = open(i,\"r\",encoding='utf-8') #opening each file\n",
    "    raw_documents.append(fin.read()) #reading \n",
    "    fin.close()\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))\n",
    "print(raw_documents[0])#showing an example document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     article_id    category  \\\n",
      "0      jan-0418  technology   \n",
      "1      jan-0027    business   \n",
      "2      jan-0631  technology   \n",
      "3      jan-2105    business   \n",
      "4      jan-3300       sport   \n",
      "5      jan-4187       sport   \n",
      "6      jan-1974       sport   \n",
      "7      jan-3666       sport   \n",
      "8      jan-2629  technology   \n",
      "9      jan-2415  technology   \n",
      "10     jan-4210       sport   \n",
      "11     jan-4789    business   \n",
      "12     jan-3452    business   \n",
      "13     jan-2428  technology   \n",
      "14     jan-4766  technology   \n",
      "15     jan-2595  technology   \n",
      "16     jan-2935    business   \n",
      "17     jan-0578    business   \n",
      "18     jan-3023  technology   \n",
      "19     jan-2356       sport   \n",
      "20     jan-1023       sport   \n",
      "21     jan-0641    business   \n",
      "22     jan-2461       sport   \n",
      "23     jan-4541       sport   \n",
      "24     jan-1259       sport   \n",
      "25     jan-4007  technology   \n",
      "26     jan-4171    business   \n",
      "27     jan-0272  technology   \n",
      "28     jan-4894       sport   \n",
      "29     jan-4504    business   \n",
      "...         ...         ...   \n",
      "1378   dec-2445  technology   \n",
      "1379   dec-4353       sport   \n",
      "1380   dec-2117  technology   \n",
      "1381   dec-3966    business   \n",
      "1382   dec-1690  technology   \n",
      "1383   dec-3207       sport   \n",
      "1384   dec-2963       sport   \n",
      "1385   dec-2023    business   \n",
      "1386   dec-2397  technology   \n",
      "1387   dec-3903  technology   \n",
      "1388   dec-3692    business   \n",
      "1389   dec-0367    business   \n",
      "1390   dec-3169  technology   \n",
      "1391   dec-0271       sport   \n",
      "1392   dec-4210       sport   \n",
      "1393   dec-0983    business   \n",
      "1394   dec-3570       sport   \n",
      "1395   dec-2331       sport   \n",
      "1396   dec-3694  technology   \n",
      "1397   dec-1576    business   \n",
      "1398   dec-4918       sport   \n",
      "1399   dec-2759  technology   \n",
      "1400   dec-4347  technology   \n",
      "1401   dec-3179    business   \n",
      "1402   dec-1456  technology   \n",
      "1403   dec-2238       sport   \n",
      "1404   dec-2334    business   \n",
      "1405   dec-2095    business   \n",
      "1406   dec-1351    business   \n",
      "1407   dec-0068       sport   \n",
      "\n",
      "                                                  title  \\\n",
      "0     21st-Century Sports: How Digital Technology Is...   \n",
      "1                      Asian quake hits European shares   \n",
      "2                        BT offers free net phone calls   \n",
      "3                     Barclays shares up on merger talk   \n",
      "4                      Barkley fit for match in Ireland   \n",
      "5                                Bellamy under new fire   \n",
      "6                     Benitez 'to launch Morientes bid'   \n",
      "7                     Benitez delight after crucial win   \n",
      "8                           Big war games battle it out   \n",
      "9                     British Library gets wireless net   \n",
      "10                    Brizzel to run AAA's in Sheffield   \n",
      "11                      Bush budget seeks deep cutbacks   \n",
      "12                       Bush to get 'tough' on deficit   \n",
      "13                         Cable offers video-on-demand   \n",
      "14                     Cabs collect mountain of mobiles   \n",
      "15                       Camera phones are 'must-haves'   \n",
      "16                      Card fraudsters 'targeting web'   \n",
      "17                    Cash gives way to flexible friend   \n",
      "18                     Cebit opens to mobile music tune   \n",
      "19                      Charvis set to lose fitness bid   \n",
      "20                                 Chelsea hold Arsenal   \n",
      "21                     Christmas sales worst since 1981   \n",
      "22                    Clijsters set for February return   \n",
      "23                                     Clyde 0-5 Celtic   \n",
      "24                     Coach Ranieri sacked by Valencia   \n",
      "25                    Confusion over high-definition TV   \n",
      "26                       Cuba winds back economic clock   \n",
      "27                              DS aims to touch gamers   \n",
      "28                    Dawson set for new Wasps contract   \n",
      "29                           Diageo to buy US wine firm   \n",
      "...                                                 ...   \n",
      "1378                   Sony PSP tipped as a 'must-have'   \n",
      "1379                   Stevens named in England line-up   \n",
      "1380                   T-Mobile bets on 'pocket office'   \n",
      "1381                   Takeover offer for Sunderland FC   \n",
      "1382                          The future in your pocket   \n",
      "1383                                 The gloves are off   \n",
      "1384                    Tigers wary of Farrell 'gamble'   \n",
      "1385                  Troubled Marsh under SEC scrutiny   \n",
      "1386                  UK gets official virus alert site   \n",
      "1387                    US blogger fired by her airline   \n",
      "1388                    US company admits Benin bribery   \n",
      "1389                  US seeks new $280bn smoker ruling   \n",
      "1390                      US woman sues over cartridges   \n",
      "1391                           Uefa approves fake grass   \n",
      "1392                   Unclear future for striker Baros   \n",
      "1393                    Venezuela reviews foreign deals   \n",
      "1394                         Vickery out of Six Nations   \n",
      "1395                  Wales make two changes for France   \n",
      "1396                   Warning over tsunami aid website   \n",
      "1397                       Weak dollar hits Online News   \n",
      "1398                   Wenger dejected as Arsenal slump   \n",
      "1399               What high-definition will do to DVDs   \n",
      "1400                 What's next for next-gen consoles?   \n",
      "1401               Why few targets are better than many   \n",
      "1402                  Wi-fi web reaches farmers in Peru   \n",
      "1403                    Woodward eyes Brennan for Lions   \n",
      "1404                  WorldCom trial starts in New York   \n",
      "1405                    Yukos accused of lying to court   \n",
      "1406                   Yukos drops banks from court bid   \n",
      "1407                      Zambia confident and cautious   \n",
      "\n",
      "                                                   body  \n",
      "0     The sporting industry has come a long way sinc...  \n",
      "1     Asian quake hits European sharesShares in Euro...  \n",
      "2     BT is offering customers free internet telepho...  \n",
      "3     Barclays shares up on merger talkShares in UK ...  \n",
      "4     England centre Olly Barkley has been passed fi...  \n",
      "5     Bellamy under new fireNewcastle boss Graeme So...  \n",
      "6     Benitez 'to launch Morientes bid'Liverpool may...  \n",
      "7     Liverpool manager Rafael Benitez admitted vict...  \n",
      "8     The arrival of new titles in the popular Medal...  \n",
      "9     Visitors to the British Library will be able t...  \n",
      "10    Ballymena sprinter Paul Brizzel will be among ...  \n",
      "11    Bush budget seeks deep cutbacksPresident Bush ...  \n",
      "12    Bush to get 'tough' on deficitUS president Geo...  \n",
      "13    Cable offers video-on-demandCable firms NTL an...  \n",
      "14    Gadgets are cheaper, smaller and more common t...  \n",
      "15    Camera phones are 'must-haves'Four times more ...  \n",
      "16    Card fraudsters 'targeting web'New safeguards ...  \n",
      "17    Cash gives way to flexible friendSpending on c...  \n",
      "18    Cebit, the world's largest hi-tech fair, has o...  \n",
      "19    Flanker Colin Charvis is unlikely to play any ...  \n",
      "20    A gripping game between Arsenal and Chelsea en...  \n",
      "21    Christmas sales worst since 1981UK retail sale...  \n",
      "22    Tennis star Kim Clijsters will make her return...  \n",
      "23    Clyde 0-5 CelticCeltic brushed aside Clyde to ...  \n",
      "24    Coach Ranieri sacked by ValenciaClaudio Ranier...  \n",
      "25    Confusion over high-definition TVNow that a cr...  \n",
      "26    Cuba winds back economic clockFidel Castro's d...  \n",
      "27    The mobile gaming industry is set to explode i...  \n",
      "28    European champions Wasps are set to offer Matt...  \n",
      "29    Diageo, the world's biggest spirits company, h...  \n",
      "...                                                 ...  \n",
      "1378  Sony PSP tipped as a 'must-have'Sony's Playsta...  \n",
      "1379  Stevens named in England line-upEngland have n...  \n",
      "1380  T-Mobile has launched its latest \"pocket offic...  \n",
      "1381  Takeover offer for Sunderland FCBob Murray, ch...  \n",
      "1382  If you are a geek or gadget fan, the next 12 m...  \n",
      "1383  The gloves are offWhen Arsenal play Manchester...  \n",
      "1384  Leicester say they will not be rushed into mak...  \n",
      "1385  Troubled Marsh under SEC scrutinyThe US stock ...  \n",
      "1386  A rapid alerting service that tells home compu...  \n",
      "1387  A US airline attendant suspended over \"inappro...  \n",
      "1388  A US defence and telecommunications company ha...  \n",
      "1389  The US Justice Department is to try to overtur...  \n",
      "1390  US woman sues over cartridgesA US woman is sui...  \n",
      "1391  Uefa approves fake grassUefa says it will allo...  \n",
      "1392  Unclear future for striker BarosLiverpool forw...  \n",
      "1393  Venezuela is to review all foreign investment ...  \n",
      "1394  England tight-head prop Phil Vickery has been ...  \n",
      "1395  Wales coach Mike Ruddock has made two changes ...  \n",
      "1396  Net users are being told to avoid a scam websi...  \n",
      "1397  Revenues at media group Online News slipped 11...  \n",
      "1398  Wenger dejected as Arsenal slumpArsenal manage...  \n",
      "1399  First it was the humble home video, then it wa...  \n",
      "1400  The next generation of video games consoles ar...  \n",
      "1401  The economic targets set out at the Lisbon sum...  \n",
      "1402  A network of community computer centres, linke...  \n",
      "1403  Woodward eyes Brennan for LionsToulouse's form...  \n",
      "1404  The trial of Bernie Ebbers, former chief execu...  \n",
      "1405  Yukos accused of lying to courtRussian oil fir...  \n",
      "1406  Russian oil company Yukos has dropped the thre...  \n",
      "1407  Zambia's technical director, Kalusha Bwalya is...  \n",
      "\n",
      "[1408 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Showing the Dataframe Created in Part 1 which has Article and Category link\n",
    "final_categories['body']=raw_documents #adding the content too to the csv\n",
    "print(final_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizing Text\n",
    "The first step in analysing unstructured documents is to split the raw text into individual tokens, each corresponding to a single term (word). But before tokenizing, we need to convert everything to lower case or else \"Car\" or \"CAR\" or \"car\" won't be the same.\n",
    "##### Case Conversion\n",
    "This is important as the computer treats the two words with different cases but same spelling as different. For eg, 'CAR' wont be same as 'car', so case conversion is required before analyzing.\n",
    "##### Lemmatizing Text\n",
    "The next step is to reduce the word to its root! For example, if we have 2 sentences:\n",
    "<ol><li> I was running in the ground</li>\n",
    "<li> I ran in the ground</li></ol>\n",
    "    Both of the sentences mean the same but one is constructed with the tense 'running' and other with 'ran'. Both the words are used to convey the same meaning. So when we store them in a document term matrix, they will occupy different places though they mean exactly the same. This is not efficient and might even affect the analysis as the words and their values will be affected. for eg. running has a score of 3 and ran has a score of 2 but if we lemmatize them these scores will be represented by their lemmatized form. As show below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokens=['running','ran']\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemma_tokens = []\n",
    "for token in tokens:\n",
    "    lemma_tokens.append(lemmatizer.lemmatize(token,pos='v'))#performing lemmatization\n",
    "print(lemma_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I have used lemmatization over stemming because in stemming sometimes it produces words which doesn't make any sense just to get the root of the word. So Lemmatization is better in that sense as it will always produce root words or similar words which have same meaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So combining, <b>Tokenizing</b>, <b>Case Conversion</b> and <b>Lemmatization</b> into one fucntion given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function which tokenizes the data, converts it to lower-case and lemmatizes it!\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def lemma_tokenizer(text):\n",
    "    #use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(text.lower())#tokenizing the text after converting it to lower-case\n",
    "    #then use NLTK to perform lemmatisation on each token\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma_tokens.append(lemmatizer.lemmatize(token)) #performing lemmatization\n",
    "    return lemma_tokens #returning the lemmatized tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the data into Training & Testing data.\n",
    "We need to split the data into training and testing data, where the training data will be used to train the model and build it well and testing to test the accuracy of the predictions made by the model. The reason for splitting the data in the first place is to avoid overfitting. As we need to make a model which is general and not specific only to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  1126\n",
      "Test dataset:  282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    final_categories['body'], #body column of DataFrame has the content of each article\n",
    "    final_categories['category'], #Category column of DataFrame has category for each of the article\n",
    "    #random_state = 1 #makes the split produce same results everytime\n",
    "    train_size=0.8, #80-20 split of the articles into training and testing data\n",
    "    test_size=0.2 #It's common-sense but have to specify or it shows warning.\n",
    ")\n",
    "print(\"Training dataset: \", X_train.shape[0])\n",
    "print(\"Test dataset: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency- Inverse Document Frequency(TF-IDF) over Document Term Matrix\n",
    "A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. But it fails if some term appears in too many documents and sometimes might affect the analysis. So it is better to include a bit more of the information which can be done by using TF-IDF matrix.\n",
    "Term Frequency(TF) of TF-IDF stands for same,term frequency but Inverse Document Frequency(IDF) will penalize terms which are too common across all the documents. \n",
    "\n",
    "<p><i>I did run the entire code using Document Term Matrix, the accuracy and even F-measure score with TF-IDF is a bit better!</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop words\n",
    "Many of the words aren't of any use and appear in all texts of english, i.e. is,the for, etc which are the STOP-WORDS. As they are present in all the documents, they aren't quite useful for analyzing the data and then classifying it. We need to remove those while creating the TF-IDF matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Low frequency words\n",
    "This is very straight forward, if a word appears very rarely or appears in a very less of the documents of same or different category, then what's the use of storing information(weights or counts) regarding that word as it won't affect the analysis in any case. We need to remove such low frequency words so such words aren't stored in the TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before processing the number of terms we have: 21294\n",
      "After processing the number of terms we have: 7685\n",
      "(1126, 7685)\n",
      "(282, 7685)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfIdf_vector = TfidfVectorizer(stop_words=\"english\", min_df = 3,tokenizer=lemma_tokenizer)\n",
    "#terms with frequency less than 3 and using lemma_tokenizer\n",
    "training_data = TfIdf_vector.fit_transform(X_train) #making a sparse TF-IDF matrix of token counts for training data\n",
    "testing_data = TfIdf_vector.transform(X_test) #making a sparse TF-IDF matrix of token counts for testing data\n",
    "TfIdf_vector2=TfidfVectorizer()#no processing included!\n",
    "training_data2 = TfIdf_vector2.fit_transform(X_train) #training data without any processing\n",
    "testing_data2 = TfIdf_vector2.transform(X_test) #testing data without any processing\n",
    "print(\"Before processing the number of terms we have:\",len(TfIdf_vector2.vocabulary_))\n",
    "print(\"After processing the number of terms we have:\",len(TfIdf_vector.vocabulary_))\n",
    "print(training_data.shape)\n",
    "print(testing_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying Models:\n",
    "The models I have explored in this notebook are:\n",
    "1. <u>Naive Bayes</u>:<br>\n",
    "Simple classifier based on counts. Requires less training data,but it assumes all features are independent which might be a problem in our case where the certain terms might be related to each other and they together might tell us more about what the article is and classifies the article better.\n",
    "2. <u>KNN</u>:<br>\n",
    "Simple but effective \"lazy\" classifier. Find most similar previous examples for which a decision has already \n",
    "been made (i.e. their nearest neighbours from the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB() #creating a model based on Naive Bayes Classification\n",
    "naive_bayes.fit(training_data, y_train) #training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sport', 'sport', 'sport', 'technology', 'business', 'business',\n",
       "       'business', 'sport', 'sport', 'business', 'business', 'technology',\n",
       "       'technology', 'technology', 'sport', 'technology', 'sport', 'sport',\n",
       "       'business', 'business', 'technology', 'business', 'technology',\n",
       "       'sport', 'technology', 'technology', 'sport', 'business', 'sport',\n",
       "       'business', 'technology', 'business', 'sport', 'business',\n",
       "       'business', 'technology', 'technology', 'technology', 'sport',\n",
       "       'technology', 'sport', 'business', 'technology', 'business',\n",
       "       'sport', 'sport', 'business', 'business', 'technology',\n",
       "       'technology', 'technology', 'technology', 'technology', 'sport',\n",
       "       'business', 'technology', 'technology', 'business', 'business',\n",
       "       'technology', 'technology', 'sport', 'sport', 'sport', 'technology',\n",
       "       'business', 'sport', 'business', 'technology', 'business',\n",
       "       'business', 'sport', 'sport', 'business', 'business', 'business',\n",
       "       'sport', 'technology', 'technology', 'business', 'sport',\n",
       "       'technology', 'sport', 'sport', 'sport', 'sport', 'business',\n",
       "       'sport', 'technology', 'sport', 'sport', 'business', 'technology',\n",
       "       'sport', 'sport', 'business', 'sport', 'business', 'technology',\n",
       "       'technology', 'business', 'business', 'technology', 'business',\n",
       "       'technology', 'sport', 'sport', 'technology', 'sport', 'technology',\n",
       "       'sport', 'sport', 'sport', 'sport', 'sport', 'technology', 'sport',\n",
       "       'business', 'sport', 'technology', 'sport', 'technology',\n",
       "       'business', 'sport', 'technology', 'sport', 'sport', 'technology',\n",
       "       'sport', 'business', 'business', 'technology', 'business',\n",
       "       'business', 'business', 'business', 'business', 'business',\n",
       "       'business', 'sport', 'sport', 'technology', 'business', 'business',\n",
       "       'sport', 'sport', 'sport', 'technology', 'business', 'technology',\n",
       "       'sport', 'business', 'business', 'business', 'technology',\n",
       "       'technology', 'business', 'business', 'technology', 'business',\n",
       "       'business', 'business', 'sport', 'sport', 'sport', 'technology',\n",
       "       'technology', 'business', 'sport', 'business', 'sport', 'business',\n",
       "       'technology', 'business', 'technology', 'technology', 'technology',\n",
       "       'sport', 'business', 'sport', 'business', 'sport', 'sport',\n",
       "       'business', 'technology', 'business', 'sport', 'sport',\n",
       "       'technology', 'business', 'sport', 'technology', 'sport',\n",
       "       'technology', 'sport', 'sport', 'sport', 'sport', 'technology',\n",
       "       'sport', 'technology', 'sport', 'technology', 'technology', 'sport',\n",
       "       'sport', 'sport', 'sport', 'sport', 'sport', 'business', 'business',\n",
       "       'sport', 'technology', 'sport', 'sport', 'sport', 'sport',\n",
       "       'business', 'sport', 'business', 'sport', 'business', 'technology',\n",
       "       'business', 'technology', 'business', 'sport', 'business',\n",
       "       'business', 'technology', 'technology', 'technology', 'sport',\n",
       "       'sport', 'sport', 'sport', 'sport', 'sport', 'business', 'sport',\n",
       "       'sport', 'technology', 'sport', 'sport', 'technology', 'technology',\n",
       "       'business', 'sport', 'technology', 'sport', 'technology',\n",
       "       'technology', 'sport', 'business', 'technology', 'business',\n",
       "       'business', 'technology', 'technology', 'business', 'technology',\n",
       "       'sport', 'business', 'technology', 'business', 'sport', 'sport',\n",
       "       'technology', 'sport', 'sport', 'sport', 'technology', 'sport',\n",
       "       'sport', 'technology', 'technology', 'business', 'technology',\n",
       "       'business', 'business', 'technology'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_predictions = naive_bayes.predict(testing_data) #giving the testing data to the model to generate predictions\n",
    "naive_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)#creating a model based on KNN Classification\n",
    "knn_model.fit(training_data, y_train) #training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sport', 'sport', 'sport', 'technology', 'sport', 'business',\n",
       "       'business', 'sport', 'sport', 'business', 'business', 'technology',\n",
       "       'technology', 'technology', 'sport', 'technology', 'sport', 'sport',\n",
       "       'business', 'business', 'technology', 'technology', 'technology',\n",
       "       'sport', 'technology', 'technology', 'sport', 'business', 'sport',\n",
       "       'business', 'technology', 'business', 'sport', 'business',\n",
       "       'business', 'technology', 'technology', 'technology', 'sport',\n",
       "       'technology', 'sport', 'business', 'technology', 'business',\n",
       "       'sport', 'sport', 'business', 'business', 'technology',\n",
       "       'technology', 'technology', 'technology', 'technology', 'sport',\n",
       "       'business', 'technology', 'technology', 'business', 'business',\n",
       "       'technology', 'technology', 'sport', 'sport', 'sport', 'technology',\n",
       "       'business', 'sport', 'business', 'technology', 'business',\n",
       "       'business', 'sport', 'sport', 'business', 'business', 'business',\n",
       "       'sport', 'technology', 'technology', 'business', 'business',\n",
       "       'technology', 'sport', 'sport', 'sport', 'sport', 'business',\n",
       "       'sport', 'technology', 'sport', 'sport', 'business', 'technology',\n",
       "       'sport', 'business', 'business', 'business', 'business',\n",
       "       'technology', 'technology', 'business', 'business', 'technology',\n",
       "       'business', 'technology', 'sport', 'sport', 'technology', 'sport',\n",
       "       'business', 'sport', 'sport', 'sport', 'sport', 'sport',\n",
       "       'technology', 'sport', 'business', 'sport', 'technology', 'sport',\n",
       "       'technology', 'business', 'sport', 'technology', 'sport', 'sport',\n",
       "       'technology', 'sport', 'business', 'business', 'technology',\n",
       "       'business', 'business', 'business', 'business', 'business', 'sport',\n",
       "       'business', 'sport', 'sport', 'technology', 'business', 'business',\n",
       "       'sport', 'sport', 'sport', 'technology', 'business', 'technology',\n",
       "       'sport', 'technology', 'business', 'business', 'technology',\n",
       "       'business', 'business', 'business', 'business', 'business',\n",
       "       'business', 'business', 'sport', 'sport', 'sport', 'technology',\n",
       "       'technology', 'business', 'sport', 'business', 'sport', 'business',\n",
       "       'technology', 'business', 'technology', 'technology', 'technology',\n",
       "       'sport', 'business', 'sport', 'business', 'sport', 'sport',\n",
       "       'business', 'technology', 'business', 'sport', 'sport',\n",
       "       'technology', 'business', 'sport', 'sport', 'sport', 'technology',\n",
       "       'sport', 'sport', 'sport', 'sport', 'technology', 'sport',\n",
       "       'technology', 'sport', 'technology', 'technology', 'sport', 'sport',\n",
       "       'sport', 'sport', 'sport', 'technology', 'business', 'business',\n",
       "       'sport', 'technology', 'sport', 'sport', 'sport', 'sport',\n",
       "       'business', 'sport', 'business', 'sport', 'business', 'business',\n",
       "       'business', 'technology', 'business', 'sport', 'business',\n",
       "       'business', 'technology', 'technology', 'technology', 'sport',\n",
       "       'sport', 'sport', 'sport', 'sport', 'sport', 'business', 'sport',\n",
       "       'sport', 'technology', 'sport', 'sport', 'technology', 'technology',\n",
       "       'business', 'sport', 'technology', 'sport', 'technology',\n",
       "       'technology', 'sport', 'business', 'technology', 'business',\n",
       "       'business', 'technology', 'technology', 'sport', 'technology',\n",
       "       'sport', 'technology', 'technology', 'business', 'sport', 'sport',\n",
       "       'technology', 'sport', 'sport', 'sport', 'technology', 'sport',\n",
       "       'sport', 'technology', 'technology', 'business', 'technology',\n",
       "       'business', 'business', 'technology'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_predictions = knn_model.predict(testing_data)#feeding the testing data to the model to generate predictions\n",
    "knn_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation of Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are number of ways to evaluate the classifiers, the prediction made by them when it comes to unseen data or new data. Better trained the model is, better are the classification predictions given by the model.<br>\n",
    "1. <u>Accuracy</u>: <br>\n",
    "Simplest measure. <b>Fraction of correct predictions</b> made by the classifier.\n",
    "2. <u>Recall Score</u>:<br>\n",
    "The recall is the ratio <b>tp / (tp + fn)</b> where tp is the number of true positives and fn the number of false negatives.In short it is the is the fraction of relevant instances(tp) that have been retrieved over the total amount of relevant instances(tp+fn).\n",
    "3. <u>Precision Score</u>:<br>\n",
    "The precision is the ratio <b>tp / (tp + fp)</b> where tp is the number of true positives and fp the number of false positives. In short it is the fraction of relevant instances(tp) among the retrieved instances(tp+fp).\n",
    "4. <u>F1 Score</u>: <br>\n",
    "The F1 score can be interpreted as a <b>weighted average of the precision and recall</b>, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "5. <u> K-Cross Fold Validation</u>:<br>\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into <b>k equal sized subsamples</b>. Of the k subsamples, a <b>single subsample</b> is retained as the <b>validation data for testing</b> the model, and the remaining <b>k − 1 subsamples</b> are used as <b>training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><b><u>Evaluation of Naive Bayes:</u></b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.971631205674\n",
      "Recall score:  0.971631205674\n",
      "Precision score:  0.971805136971\n",
      "F1 score:  0.971660551679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, naive_predictions))\n",
    "print(\"Recall score: \", recall_score(y_test,naive_predictions, average = 'weighted'))\n",
    "print(\"Precision score: \", precision_score(y_test, naive_predictions, average = 'weighted'))\n",
    "print(\"F1 score: \", f1_score(y_test, naive_predictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### K-Cross Fold Evaluation of Naive Bayes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95070423  0.95714286]\n",
      "Naive Bayes: Mean cross-validation accuracy = 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "naive_acc_scores = cross_val_score(naive_bayes, testing_data, y_test, cv=2, scoring=\"accuracy\")\n",
    "#naive_bayes attribute specifies the model, data, target labels, cv says number of folds. Scoring is based on Accuracy\n",
    "print(naive_acc_scores)\n",
    "print(\"Naive Bayes: Mean cross-validation accuracy = %.2f\" % naive_acc_scores.mean())#printing mean of all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95062384  0.95708781]\n",
      "Naive Bayes: Mean cross-validation f1-score = 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "naive_f1_scores = cross_val_score(naive_bayes, testing_data, y_test, cv=2, scoring=\"f1_weighted\")\n",
    "#naive_bayes attribute specifies the model, data, target labels, cv says number of folds. Scoring is based on Accuracy\n",
    "print(naive_f1_scores)\n",
    "print(\"Naive Bayes: Mean cross-validation f1-score = %.2f\" % naive_f1_scores.mean())#printing mean of all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95070423  0.95714286]\n",
      "Naive Bayes: Mean cross-validation Recall-score = 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "naive_recall_scores = cross_val_score(naive_bayes, testing_data, y_test, cv=2, scoring=\"recall_weighted\")\n",
    "#naive_bayes attribute specifies the model, data, target labels, cv says number of folds. Scoring is based on Accuracy\n",
    "print(naive_recall_scores)\n",
    "print(\"Naive Bayes: Mean cross-validation Recall-score = %.2f\" % naive_recall_scores.mean())#printing mean of all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95145768  0.9595696 ]\n",
      "Naive Bayes: Mean cross-validation Precision-score = 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "naive_prec_scores = cross_val_score(naive_bayes, testing_data, y_test, cv=2, scoring=\"precision_weighted\")\n",
    "#naive_bayes attribute specifies the model, data, target labels, cv says number of folds. Scoring is based on Accuracy\n",
    "print(naive_prec_scores)\n",
    "print(\"Naive Bayes: Mean cross-validation Precision-score = %.2f\" % naive_prec_scores.mean())#printing mean of all scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><b><u>Evaluation of KNN:</u></b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.946808510638\n",
      "Recall score:  0.946808510638\n",
      "Precision score:  0.946920392915\n",
      "F1 score:  0.946846773457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, knn_predictions))\n",
    "print(\"Recall score: \", recall_score(y_test, knn_predictions, average = 'weighted'))\n",
    "print(\"Precision score: \", precision_score(y_test, knn_predictions, average = 'weighted'))\n",
    "print(\"F1 score: \", f1_score(y_test, knn_predictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### K-cross Fold Evaluation of KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93661972  0.94285714]\n",
      "KNN: Mean cross-validation accuracy = 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "acc_scores_knn = cross_val_score(knn_model, testing_data, y_test, cv=2, scoring=\"accuracy\")\n",
    "#knn_model specifies model used, data used, target labels used, cv says number of folds. Scoring is based on Accuracy.\n",
    "print(acc_scores_knn)\n",
    "print(\"KNN: Mean cross-validation accuracy = %.2f\" % acc_scores_knn.mean()) #printing mean of all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93666885  0.94163228]\n",
      "KNN: Mean cross-validation F1-score = 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "f1_scores_knn = cross_val_score(knn_model, testing_data, y_test, cv=2, scoring=\"f1_weighted\")\n",
    "#knn_model specifies model used, data used, target labels used, cv says number of folds. Scoring is based on Accuracy.\n",
    "print(f1_scores_knn)\n",
    "print(\"KNN: Mean cross-validation F1-score = %.2f\" % f1_scores_knn.mean()) #printing mean of all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93841471  0.9447695 ]\n",
      "KNN: Mean cross-validation Precision-score = 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "prec_scores_knn = cross_val_score(knn_model, testing_data, y_test, cv=2, scoring=\"precision_weighted\")\n",
    "#knn_model specifies model used, data used, target labels used, cv says number of folds. Scoring is based on Accuracy.\n",
    "print(prec_scores_knn)\n",
    "print(\"KNN: Mean cross-validation Precision-score = %.2f\" % prec_scores_knn.mean()) #printing mean of all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93661972  0.94285714]\n",
      "KNN: Mean cross-validation Recall-score = 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "recall_scores_knn = cross_val_score(knn_model, testing_data, y_test, cv=2, scoring=\"recall_weighted\")\n",
    "#knn_model specifies model used, data used, target labels used, cv says number of folds. Scoring is based on Accuracy.\n",
    "print(recall_scores_knn)\n",
    "print(\"KNN: Mean cross-validation Recall-score = %.2f\" % recall_scores_knn.mean()) #printing mean of all scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Conclusion and  Reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAFVCAYAAACZyeiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAF3hJREFUeJzt3X2UVPWd5/HPh25RDAQDdAB5EDfi\nA7gShHiycbOaMcnB5ODDrqi9ZogzQbJRk5ljdJbEHcYxToYkbtxj1GQ8jppJBETRsccwsFnHbJ40\nIwYwPPjAIoZW0RYMURNhmv7uH3VbKkXTXUJ1V+P3/TqnDvf+7q9u/e6PW5/63XurbjsiBADZDKh3\nAwCgHgg/ACkRfgBSIvwApET4AUiJ8AOQUmO9Xvjxxx9/b2Nj422STlS+EO6QtLa9vX3OtGnTXq53\nY4CM6hZ+jY2Nt40aNeqEpqamVwcMGJDqy4YdHR1ua2ubtHXr1tsknVXv9gAZ1XPEdWJTU9NvswWf\nJA0YMCCampp2qDTqBVAH9Qy/ARmDr1Ox7dkO94F+I/Wbz/a0Sy65ZGzn/Pz580deccUVR3b3nLvu\numvol7/85VG93zoAvalu5/wqTZj3g2m1XN/mBZ98vKc6AwcOjGXLlr3nxRdf3Dp69Oj2atZ70UUX\n7ZC044AbCKCuUo/8GhoaYvbs2W1f/epXR1YuW7hw4dCTTjrp+BNOOGHShz70oWO3bNnSKEk33njj\n8NmzZ4/ftm1bw5gxY/797t27JUmvvfbagFGjRp20c+dOr1u37tAPf/jDEydPnnzCtGnTjlu1atVh\nfbxpAHqQOvwk6aqrrnr5vvvuG7Zt27aG8vKPfexjr69evfrJDRs2rD/vvPO2X3vttX9wqDt8+PDd\nxx9//O+WLVs2RJIWL1489LTTTttx6KGHxpw5c4665ZZbfr1u3boN3/jGN1o/97nPje/LbQLQs35z\n2Fsvw4YN65g1a9a2BQsWvHfQoEEdneXPPvvswHPOOWdsW1vbIbt27Rowbty4nZXPnTVr1quLFi16\nz8yZM19bsmTJsEsvvbRtx44dA1atWjV41qxZ7+ust2vXLvfV9gCoTvqRnyR96UtfemnhwoUj3njj\njbf64/LLLx9/6aWXvvz000+vv+mmm57buXPnXn3V3Nz8mx/96EdDX3rppYa1a9cePnPmzN/u3r1b\nQ4YMaX/yySfXdz42bdq0rm+3CEBPCD9JI0eO3D1z5sxXFy5cOKKz7LXXXmsYP378v0nSnXfeObyr\n5w0dOrRjypQpb3z2s58df8YZZ+xobGzUsGHDOsaOHbvr9ttvf48kdXR06JFHHhnUN1sCoFqEX+Hq\nq6/e+pvf/KaxbP6F5ubm902bNu244cOH7/NK8Pnnn//qAw88MKy5uXl7Z9miRYs23XHHHSOOO+64\nSRMnTpy8dOnSI3q7/QDeHtfrNvZr1qzZPGXKlFfq8uL9xJo1a0ZMmTJlQr3bAWTEyA9ASoQfgJQI\nPwApEX4AUiL8AKRE+AFIKXX4HX744VM7p+++++6hRx111InPPPPMwCuuuOLIQYMGTX3++ecbu6q7\nP7fCAtC/9J/f9l4ztKa3tNI1O3q8pVWnBx54YMiVV145bvny5c9MnDhxlyQdccQR7dddd93Ib3/7\n289X1t+fW2EB6F9Sj/wkafny5YMvu+yyCS0tLRsnT5781s0Lmpubt7W0tAx76aWXGiqf092tsAAc\nHFKH365du3zBBRccs3Tp0o1Tp059s3zZ4MGDdzc3N7+yYMGCLgNuX7fCAnBwSB1+hxxySJx88smv\nf+c73xnR1fJ58+a9vGTJkuHbt2/fq5/Kb4XV+y0FUGupw8+2WlpaNq1evfpd8+bN2+vvcowYMWL3\nueeeu/3666/vMuC6uhUWgIND+jftkCFDOpYvX/7MvffeO/yGG27YawR49dVXv/Td7363affu3Xvd\nkLSrW2EBODikDz+pFGLLly9/+vrrrx/9/e9//w9uPzV69Oj2M88889V93Y258lZYAA4O3NKqjril\nFVA/jPwApET4AUiJ8AOQUj3Dr6OjoyPtn3Qstr2jx4oAekU9w29tW1vb0IwB2NHR4ba2tqGS1ta7\nLUBWdfuKRnt7+5ytW7fetnXr1hOV7/C7Q9La9vb2OfVuCJBV3b7qAgD1lG3EBQCSqgg/27fbftl2\nl+enXHKj7Y22n7B9cu2bCQC1Vc3I705JM7pZfqakicVjrqRvH3izAKB39XjBIyJ+bHtCN1XOlvQP\nUTp5+KjtI2yPjogXu1vviBEjYsKE7lYLAG/f448//kpENPVUrxZXe8dI2lI231qU7RV+tueqNDrU\n+PHjtXLlyhq8PADsYfu5aurV4oJHV9/T6/ISckTcGhHTI2J6U1OPwQwAvaYW4dcqaVzZ/FhJL9Rg\nvQDQa2oRfi2SZhdXfT8oaUdP5/sAoN56POdne5Gk0yWNsN0q6a8kHSJJEfEdScskfULSRkm/k/Qn\nvdVYAKiVaq72NvewPCRdVrMWAUAf4BceAFIi/ACkRPgBSInwA5AS4QcgJcIPQEoH1R/bnjDvB/Vu\nQs1tXvDJ/XoefVHyTuwHib7otL/vj2ow8gOQEuEHICXCD0BKB9U5v82H/dd6N6EX7NivZ9EXJe/M\nfpDoi0779/6oBiM/ACkRfgBSIvwApET4AUiJ8AOQEuEHICXCD0BKhB+AlAg/ACkRfgBSIvwApET4\nAUiJ8AOQEuEHICXCD0BKhB+AlAg/ACkRfgBSIvwApET4AUiJ8AOQEuEHICXCD0BKhB+AlAg/ACkR\nfgBSIvwApET4AUiJ8AOQEuEHICXCD0BKhB+AlAg/ACkRfgBSqir8bM+w/ZTtjbbndbF8vO2Hba+y\n/YTtT9S+qQBQOz2Gn+0GSTdLOlPSJEnNtidVVPsfkpZExFRJF0q6pdYNBYBaqmbkd4qkjRGxKSJ2\nSVos6eyKOiHp3cX0UEkv1K6JAFB71YTfGElbyuZbi7Jy10j6lO1WScskfb6rFdmea3ul7ZVtbW37\n0VwAqI1qws9dlEXFfLOkOyNirKRPSPqe7b3WHRG3RsT0iJje1NT09lsLADVSTfi1ShpXNj9Wex/W\nfkbSEkmKiEckHSZpRC0aCAC9oZrwe0zSRNtH2x6o0gWNloo6v5Z0hiTZPkGl8OO4FkC/1WP4RUS7\npMslrZC0QaWruutsX2v7rKLaFyVdYnuNpEWSLo6IykNjAOg3GqupFBHLVLqQUV42v2x6vaRTa9s0\nAOg9/MIDQEqEH4CUCD8AKRF+AFIi/ACkRPgBSInwA5AS4QcgJcIPQEqEH4CUCD8AKRF+AFIi/ACk\nRPgBSInwA5AS4QcgJcIPQEqEH4CUCD8AKRF+AFIi/ACkRPgBSInwA5AS4QcgJcIPQEqEH4CUCD8A\nKRF+AFIi/ACkRPgBSInwA5AS4QcgJcIPQEqEH4CUCD8AKRF+AFIi/ACkRPgBSInwA5AS4QcgJcIP\nQEqEH4CUCD8AKRF+AFIi/ACkVFX42Z5h+ynbG23P20ed822vt73O9sLaNhMAaquxpwq2GyTdLOlj\nklolPWa7JSLWl9WZKOlLkk6NiFdtv7e3GgwAtVDNyO8USRsjYlNE7JK0WNLZFXUukXRzRLwqSRHx\ncm2bCQC1VU34jZG0pWy+tSgrd6ykY23/zPajtmd0tSLbc22vtL2yra1t/1oMADVQTfi5i7KomG+U\nNFHS6ZKaJd1m+4i9nhRxa0RMj4jpTU1Nb7etAFAz1YRfq6RxZfNjJb3QRZ0HIuLfIuJZSU+pFIYA\n0C9VE36PSZpo+2jbAyVdKKmlos4/SvqIJNkeodJh8KZaNhQAaqnH8IuIdkmXS1ohaYOkJRGxzva1\nts8qqq2QtM32ekkPS7oqIrb1VqMB4ED1+FUXSYqIZZKWVZTNL5sOSVcUDwDo9/iFB4CUCD8AKRF+\nAFIi/ACkRPgBSInwA5AS4QcgJcIPQEqEH4CUCD8AKRF+AFIi/ACkRPgBSInwA5AS4QcgJcIPQEqE\nH4CUCD8AKRF+AFIi/ACkRPgBSInwA5AS4QcgJcIPQEqEH4CUCD8AKRF+AFIi/ACkRPgBSInwA5AS\n4QcgJcIPQEqEH4CUCD8AKRF+AFIi/ACkRPgBSInwA5AS4QcgJcIPQEqEH4CUCD8AKRF+AFIi/ACk\nRPgBSKmq8LM9w/ZTtjfantdNvfNsh+3ptWsiANRej+Fnu0HSzZLOlDRJUrPtSV3UGyLpC5J+UetG\nAkCtVTPyO0XSxojYFBG7JC2WdHYX9b4i6euS3qxh+wCgV1QTfmMkbSmbby3K3mJ7qqRxEfFgdyuy\nPdf2Stsr29ra3nZjAaBWqgk/d1EWby20B0i6QdIXe1pRRNwaEdMjYnpTU1P1rQSAGqsm/FoljSub\nHyvphbL5IZJOlPQj25slfVBSCxc9APRn1YTfY5Im2j7a9kBJF0pq6VwYETsiYkRETIiICZIelXRW\nRKzslRYDQA30GH4R0S7pckkrJG2QtCQi1tm+1vZZvd1AAOgNjdVUiohlkpZVlM3fR93TD7xZANC7\n+IUHgJQIPwApEX4AUiL8AKRE+AFIifADkBLhByAlwg9ASoQfgJQIPwApEX4AUiL8AKRE+AFIifAD\nkBLhByAlwg9ASoQfgJQIPwApEX4AUiL8AKRE+AFIifADkBLhByAlwg9ASoQfgJQIPwApEX4AUiL8\nAKRE+AFIifADkBLhByAlwg9ASoQfgJQIPwApEX4AUiL8AKRE+AFIifADkBLhByAlwg9ASoQfgJQI\nPwApEX4AUiL8AKRE+AFIqarwsz3D9lO2N9qe18XyK2yvt/2E7YdsH1X7pgJA7fQYfrYbJN0s6UxJ\nkyQ1255UUW2VpOkRcZKkeyV9vdYNBYBaqmbkd4qkjRGxKSJ2SVos6ezyChHxcET8rph9VNLY2jYT\nAGqrmvAbI2lL2XxrUbYvn5H0zwfSKADobY1V1HEXZdFlRftTkqZLOm0fy+dKmitJ48ePr7KJAFB7\n1Yz8WiWNK5sfK+mFykq2PyrpaklnRcTOrlYUEbdGxPSImN7U1LQ/7QWAmqgm/B6TNNH20bYHSrpQ\nUkt5BdtTJf2dSsH3cu2bCQC11WP4RUS7pMslrZC0QdKSiFhn+1rbZxXVviFpsKR7bK+23bKP1QFA\nv1DNOT9FxDJJyyrK5pdNf7TG7QKAXsUvPACkRPgBSInwA5AS4QcgJcIPQEqEH4CUCD8AKRF+AFIi\n/ACkRPgBSInwA5AS4QcgJcIPQEqEH4CUCD8AKRF+AFIi/ACkRPgBSInwA5AS4QcgJcIPQEqEH4CU\nCD8AKRF+AFIi/ACkRPgBSInwA5AS4QcgJcIPQEqEH4CUCD8AKRF+AFIi/ACkRPgBSInwA5AS4Qcg\nJcIPQEqEH4CUCD8AKRF+AFIi/ACkRPgBSInwA5AS4QcgJcIPQEpVhZ/tGbafsr3R9rwulh9q++5i\n+S9sT6h1QwGglnoMP9sNkm6WdKakSZKabU+qqPYZSa9GxDGSbpD0tVo3FABqqZqR3ymSNkbEpojY\nJWmxpLMr6pwt6bvF9L2SzrDt2jUTAGqrmvAbI2lL2XxrUdZlnYhol7RD0vBaNBAAekNjFXW6GsHF\nftSR7bmS5hazr9t+qorXr4cRkl7pk1f6634/QKYv9qAv9uibvti/fjiqmkrVhF+rpHFl82MlvbCP\nOq22GyUNlbS9ckURcaukW6tpWD3ZXhkR0+vdjv6AvtiDvtjjndAX1Rz2PiZpou2jbQ+UdKGkloo6\nLZI+XUyfJ+lfImKvkR8A9Bc9jvwiot325ZJWSGqQdHtErLN9raSVEdEi6e8lfc/2RpVGfBf2ZqMB\n4EBVc9iriFgmaVlF2fyy6Tclzapt0+qq3x+a9yH6Yg/6Yo+Dvi/M0SmAjPh5G4CU3hHhZ/tc22H7\n+Hq3pR5s77a9uuwxwfZw2w/bft32TfVuY18q64+1tu+xfXgN1jnd9o3dLD/S9r0H+jp9raKv/sn2\nETVe/8Wd+5/ta2xfWcv1H4h3RPhJapb0U/XihZbiZ3791e8j4v1lj82S3pT0l5L6dGcrvupUb539\ncaKkXZL+W/lCl7ytfT8iVkbEF7pZ/kJEnLd/za2r8r7aLumyejeorxz04Wd7sKRTVfp98YVl5X9h\n+1e219heUJQdY/v/FGW/tP0+26fbfrDseTfZvriY3mx7vu2fSppl+xLbjxXPX9o5orA90vb9Rfka\n2x+y/RXbf1a23r+xvc83T61FxBsR8VOVQnCfbE+2/a/Fp/8TticW5bOL+TW2v1eUHWX7oaL8Idvj\ni/I7bX/T9sOSvmb7XbZvL/pqle2zu3utXvYTSccUo+ENtm+R9EtJ42x/3PYjxb5wT7EvyfYHbP+8\n2PZ/tT2kfD+xfVrZKHtVsXyC7bXF8sNs31Hsf6tsf6Qov9j2fbaX237G9tf7YPvfjkdU9ust21cV\n/4dP2P7rsvKu9o2ZLt3UZFXxHhtZh/a/PRFxUD8kfUrS3xfTP5d0sko3Yfi5pMOL8mHFv7+QdG4x\nfZikwyWdLunBsvXdJOniYnqzpL8oWza8bPo6SZ8vpu+W9OfFdINKX/KeIOmXRdkASf+v/Pk17oPd\nklYXj/srll0s6aZunvstSRcV0wMlDZI0WdJTkkZU9N8/Sfp0Mf2nkv6xmL5T0oOSGor5r0r6VDF9\nhKSnJb2rq9fqpf54vfi3UdIDkj5X/H90SPpgsWyEpB9Lelcx/98lzS/atUnSB4rydxfreWs/Kfrh\n1GJ6cLF8gqS1RdkXJd1RTB8v6dfF/nZxse6hxfxzksbV+f3T2VcNku6RNKOY/7hKV3Rd7L8PSvpP\n3ewb79GeC6hzJP3Pyv1P0jWSrqzn9pY/+sMhyoFqlvS/iunFxfwAlXa+30lSRGy3PUTSmIi4vyh7\nU5Lc8/0X7i6bPtH2dSq9oQer9N1HSfojSbOL9e5W6bfNO2xvsz1V0khJqyJi24FsaDd+HxHv38/n\nPiLpattjJd0XEc/Y/iNJ90bEK1Kp/4q6/0HSfy6mvyepfORyT7HtUumNc1bZ+Z3DJI3v6rX2s809\nGWR7dTH9E5W+h3qkpOci4tGi/IMq3aXoZ8U+MLBo33GSXoyIxyQpIn4r7bWf/EzSN23fVWxHa8Xy\n/6hS0CsinrT9nKRji2UPRcSOYp3rVfopVvlv5/taZ19NkPS4pB8W5R8vHquK+cGSJkqaoq73jbGS\n7rY9WqW+fLZPWn8ADurwsz1cpeA50Xao9OkVkpaqut8fS1K7/vDw/7CK5W+UTd8p6ZyIWFMcGp/e\nQxNvU+mTb5Sk23uo2ydsnyvpr4rZORGx0PYvJH1S0grbc1Tqq2q+A1Vep7yfLOm/RETlb7c3VL5W\nRPzLfm1I9/b6MCjCqbKNP4yI5op6J6mHbY+IBbZ/IOkTkh61/VH94emF7j5Rd5ZN71b934O/j4j3\n2x6q0ujuMkk3qrQNfxsRf1deuTh101X/fEvSNyOixfbpKo3y+rWD/ZzfeZL+ISKOiogJETFOpU+c\n7ZL+tOyc3LDiE7zV9jlF2aHF8uckTSrmh0o6o5vXGyLpRduHSLqorPwhlQ6tZLvB9ruL8vslzZD0\nAe0ZJdZVRNwfey6MrLT97yRtiogbVfqZ4kkqbc/5xYeLbA8rnv5z7TmvepFKF5m6skLS510kTjH6\n1T5eq14elXSq7WOKth1u+1hJT0o60vYHivIhrriIY/t9EfGriPiapJUqHdqW+7GK/aNY53iVDhX7\nrWI0+gVJVxb79wqV3kOd50HH2H6v9r1vDJX0fDH9aR0EDvbwa1YpYMotVekQp0XSymJI33n49ceS\nvmD7CZXeyKMiYoukJZKekHSX9gzzu/KXKp03/KFKb5JOfybpI7Z/pdKhw2RJitL9Dx+WtKTskLDP\n2N4s6ZuSLrbd6r1vQitJF0haW/TT8Sp9mKyT9DeS/q/tNcU6pNKb40+K/vtjlba7K1+RdIikJ4qL\nAF/Z12sd6Dbur4hoU2lUvqjYnkclHV/8n10g6VvFtv9Qex8N/LlLXw1ZI+n3kv65YvktkhqK/eFu\nlc4h71Q/FxGrJK2RdGFE/G9JCyU9UmzHvZKGdLNvXCPpHts/UV/d+eYA8QuPXuTS1yl+KWlWL57f\nArAfDvaRX79VjLI2qnSCm+AD+hlGfgBSYuQHICXCD0BKhB+AlAg/ACkRfgBSIvwApPT/AYqpq1jO\nYepyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28af0cce828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(5,5))\n",
    "#creating dictionaries of the scores for naive and knn\n",
    "naive_scores={'Accuracy':naive_acc_scores.mean(),'F1-scores':naive_f1_scores.mean(),'Precision':naive_prec_scores.mean(),'Recall':naive_recall_scores.mean()}\n",
    "knn_scores = {'Accuracy':acc_scores_knn.mean(),'F1-scores':f1_scores_knn.mean(),'Precision':prec_scores_knn.mean(),'Recall':recall_scores_knn.mean()}\n",
    "#plotting naive\n",
    "plt.bar(range(len(naive_scores)), list(naive_scores.values()), align='center',label='Naive')\n",
    "plt.xticks(range(len(naive_scores)), list(naive_scores.keys()))\n",
    "#plotting knn\n",
    "plt.bar(range(len(knn_scores)), list(knn_scores.values()), align='center',label='KNN')\n",
    "plt.xticks(range(len(knn_scores)), list(knn_scores.keys()))\n",
    "#formatting the legend\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=1, borderaxespad=0.)\n",
    "#graph display!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Accuracy was given first priority as we knew already, the class labels and the article to be classified(supervised learning).\n",
    "2. The accuracy,f1-score,precision,recall of Naive Bayes is all time higher compared to KNN. Naive Bayes even had an accuracy of 1.0 in one of the cross fold evaluations.\n",
    "3. For the given data, Naive Bayes is a better classifier compared to KNN.\n",
    "4. TF-IDF is better compared to Document term matrix as all of the above scores were low compared to the ones achieved through TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
